{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.model_selection import cross_val_score\n",
    "import torch\n",
    "import transformers as ppb\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(r\"C:\\Users\\WANGYILIN\\Desktop\\ALL_lesson\\ML_foundation\\22201960.csv\",encoding=\"latin-1\")\n",
    "train_data = pd.read_csv(r\"C:\\Users\\WANGYILIN\\Desktop\\ALL_lesson\\ML foundation\\train.csv\",encoding=\"latin-1\")\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7af778c2e0d84f3580487276ca1cd900",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)solve/main/vocab.txt:   0%|          | 0.00/232k [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c31eaada622a459fba971715956a120a",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)okenizer_config.json:   0%|          | 0.00/28.0 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3f3748d5a00a4b1abf0fd9864cf38863",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading (…)lve/main/config.json:   0%|          | 0.00/483 [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b031181d2d7a447ab3790f1b6deb3b56",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Downloading model.safetensors:   0%|          | 0.00/268M [00:00<?, ?B/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at distilbert-base-uncased were not used when initializing DistilBertModel: ['vocab_layer_norm.bias', 'vocab_transform.weight', 'vocab_transform.bias', 'vocab_projector.bias', 'vocab_layer_norm.weight']\n",
      "- This IS expected if you are initializing DistilBertModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing DistilBertModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "# For DistilBERT:\n",
    "model_class, tokenizer_class, pretrained_weights = (ppb.DistilBertModel, ppb.DistilBertTokenizer, 'distilbert-base-uncased')\n",
    "\n",
    "## Want BERT instead of distilBERT? Uncomment the following line:\n",
    "#model_class, tokenizer_class, pretrained_weights = (ppb.BertModel, ppb.BertTokenizer, 'bert-base-uncased')\n",
    "\n",
    "# Load pretrained model/tokenizer\n",
    "tokenizer = tokenizer_class.from_pretrained(pretrained_weights)\n",
    "model = model_class.from_pretrained(pretrained_weights)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "best parameters:  {'C': 15.789557894736841}\n",
      "best scrores:  0.9811746392958168\n",
      "Test Accuracy:  0.9797809604043808\n"
     ]
    }
   ],
   "source": [
    "# 分词并对文本进行编码\n",
    "tokenized = train_data['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "# 找到最大长度\n",
    "max_len = max(map(len, tokenized))\n",
    "\n",
    "# Padding\n",
    "padded = np.array([i + [0]*(max_len-len(i)) for i in tokenized.values])\n",
    "\n",
    "# 创建attention_mask\n",
    "attention_mask = np.where(padded != 0, 1, 0)\n",
    "\n",
    "# 将padded和attention_mask转化为torch.Tensor\n",
    "input_ids = torch.LongTensor(padded)\n",
    "attention_mask = torch.LongTensor(attention_mask)\n",
    "\n",
    "# 提取特征\n",
    "with torch.no_grad():\n",
    "    last_hidden_states = model(input_ids, attention_mask=attention_mask)\n",
    "\n",
    "# 提取[CLS]对应的向量\n",
    "features = last_hidden_states[0][:,0,:].numpy()\n",
    "\n",
    "# 准备标签\n",
    "labels = train_data['category']\n",
    "\n",
    "# 划分训练集和测试集\n",
    "train_features, test_features, train_labels, test_labels = train_test_split(features, labels)\n",
    "\n",
    "# 设置待搜索的参数范围\n",
    "parameters = {'C': np.linspace(0.0001, 100, 20)}\n",
    "\n",
    "# 进行网格搜索以找到最佳参数\n",
    "grid_search = GridSearchCV(LogisticRegression(), parameters)\n",
    "grid_search.fit(train_features, train_labels)\n",
    "\n",
    "# 打印最佳参数和最佳分数\n",
    "print('best parameters: ', grid_search.best_params_)\n",
    "print('best scrores: ', grid_search.best_score_)\n",
    "\n",
    "# 使用找到的最佳参数训练模型\n",
    "lr_clf = LogisticRegression(C = grid_search.best_params_['C'])\n",
    "lr_clf.fit(train_features, train_labels)\n",
    "\n",
    "# 测试模型\n",
    "print(\"Test Accuracy: \", lr_clf.score(test_features, test_labels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0           0\n",
      "headline             1\n",
      "short_description    0\n",
      "date                 0\n",
      "category             0\n",
      "dtype: int64\n",
      "Unnamed: 0           0\n",
      "headline             0\n",
      "short_description    0\n",
      "date                 0\n",
      "category             0\n",
      "dtype: int64\n",
      "['PARENTING' 'THE WORLDPOST' 'PARENTING' ... 'PARENTING' 'PARENTING'\n",
      " 'PARENTING']\n",
      "               precision    recall  f1-score   support\n",
      "\n",
      "    PARENTING       0.98      0.99      0.98      1189\n",
      "THE WORLDPOST       0.96      0.95      0.95       394\n",
      "\n",
      "     accuracy                           0.98      1583\n",
      "    macro avg       0.97      0.97      0.97      1583\n",
      " weighted avg       0.98      0.98      0.98      1583\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 读取测试数据\n",
    "test_data = pd.read_csv(r\"C:\\Users\\WANGYILIN\\Desktop\\ALL_lesson\\ML foundation\\test.csv\",encoding=\"latin-1\")\n",
    "\n",
    "print(test_data.isnull().sum())\n",
    "test_data = test_data.fillna(\"\")\n",
    "print(test_data.isnull().sum())\n",
    "\n",
    "test_data['text'] = test_data['headline'] + \" \" + test_data['short_description']\n",
    "\n",
    "# 对测试数据进行分词并编码\n",
    "tokenized_test = test_data['text'].apply((lambda x: tokenizer.encode(x, add_special_tokens=True)))\n",
    "\n",
    "# 找到最大长度，注意这里应与训练数据中的最大长度保持一致\n",
    "max_len_test = max(map(len, tokenized_test))\n",
    "\n",
    "# 对测试数据进行Padding\n",
    "padded_test = np.array([i + [0]*(max_len_test-len(i)) for i in tokenized_test.values])\n",
    "\n",
    "# 创建attention_mask\n",
    "attention_mask_test = np.where(padded_test != 0, 1, 0)\n",
    "\n",
    "# 将padded和attention_mask转化为torch.Tensor\n",
    "input_ids_test = torch.LongTensor(padded_test)\n",
    "attention_mask_test = torch.LongTensor(attention_mask_test)\n",
    "\n",
    "# 提取特征\n",
    "with torch.no_grad():\n",
    "    last_hidden_states_test = model(input_ids_test, attention_mask=attention_mask_test)\n",
    "\n",
    "# 提取[CLS]对应的向量作为特征\n",
    "features_test = last_hidden_states_test[0][:,0,:].numpy()\n",
    "\n",
    "# 使用训练好的模型进行预测\n",
    "test_pred = lr_clf.predict(features_test)\n",
    "\n",
    "# 打印预测结果\n",
    "print(test_pred)\n",
    "# 使用训练好的模型进行预测\n",
    "from sklearn.metrics import classification_report\n",
    "# 进行评估\n",
    "y_test = test_data['category']\n",
    "\n",
    "print(classification_report(y_test, test_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Unnamed: 0           0\n",
      "headline             0\n",
      "short_description    0\n",
      "date                 0\n",
      "category             0\n",
      "text                 0\n",
      "dtype: int64\n",
      "Unnamed: 0           0\n",
      "headline             0\n",
      "short_description    0\n",
      "date                 0\n",
      "category             0\n",
      "text                 0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "print(train_data.isnull().sum())\n",
    "train_data = train_data.fillna(\"\")\n",
    "print(train_data.isnull().sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
